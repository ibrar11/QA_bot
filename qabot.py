from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams
from ibm_watsonx_ai.foundation_models.utils.enums import DecodingMethods
from ibm_watsonx_ai.metanames import EmbedTextParamsMetaNames
from ibm_watsonx_ai import Credentials
from langchain_ibm import WatsonxLLM, WatsonxEmbeddings
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.document_loaders import PyPDFLoader
from langchain.chains import RetrievalQA
from huggingface_hub import HfFolder
import gradio as gr
# You can use this section to suppress warnings generated by your code:
def warn(*args, **kwargs):
    pass
import warnings
warnings.warn = warn
warnings.filterwarnings('ignore')

def get_llm():
    model_id = 'ibm/granite-3-8b-instruct'
    parameters = {
        GenParams.DECODING_METHOD: DecodingMethods.GREEDY,
        GenParams.MIN_NEW_TOKENS: 130,
        GenParams.MAX_NEW_TOKENS: 256,
        GenParams.TEMPERATURE: 0.5
    }
    project_id = "9a84ef08-3951-4195-bbc4-90d096773e14"
    watsonx_llm = WatsonxLLM(
        model_id=model_id,
        url="https://jp-tok.ml.cloud.ibm.com",
        apikey="6DXNLN4Ipg-9w-t6UYKNpZ331ThpAyaqmgEnXJjWYoRT",
        project_id=project_id,
        params=parameters,
    )
    return watsonx_llm

def document_loader(file):
    loader = PyPDFLoader(file.name)
    loaded_document = loader.load()
    return loaded_document

def text_splitter(data):
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=0)
    chunks = text_splitter.split_documents(data)
    return chunks

def huggingFace_embedding():
    # embed_params = {
    #     EmbedTextParamsMetaNames.TRUNCATE_INPUT_TOKENS: 512
    # }

    # watsonx_embedding = WatsonxEmbeddings(
    #     model_id="multilingual-e5-large",
    #     url="https://jp-tok.ml.cloud.ibm.com",
    #     apikey="6DXNLN4Ipg-9w-t6UYKNpZ331ThpAyaqmgEnXJjWYoRT",
    #     project_id="9a84ef08-3951-4195-bbc4-90d096773e14",
    #     params=embed_params
    # )
    huggingFace_embedding = HuggingFaceEmbeddings()
    return huggingFace_embedding

def vector_database(chunks):
    embedding_model = huggingFace_embedding()
    vectordb = Chroma.from_documents(chunks,embedding_model)
    return vectordb

def retriever(file):
    splits = document_loader(file)
    chunks = text_splitter(splits)
    vectordb = vector_database(chunks)
    retriever = vectordb.as_retriever()
    return retriever

def retriever_qa(file, query):
    llm = get_llm()
    retriever_obj = retriever(file)
    qa = RetrievalQA.from_chain_type(
        llm = llm,
        retriever = retriever_obj,
        return_source_documents = False
    )
    response = qa.invoke(query)
    return response['result']

rag_application = gr.Interface(
    fn=retriever_qa,
    allow_flagging="never",
    inputs=[
        gr.File(
            label="Upload PDF File",
            file_count="single",
            file_types=['.pdf'],
            type="filepath"
        ),
        gr.Textbox(
            label="Input Query",
            lines=2,
            placeholder="Type your question here..."
        )
    ],
    outputs=gr.Textbox(label="Answer"),
    title="Watsonx RAG PDF Chatbot",
    description="Upload a PDF document and ask any question. The chatbot will try to answer using the provided document."
)

rag_application.launch(server_name="0.0.0.0", server_port=7860, share=True)